<p align="center"><img width="425" src="http://luispereiralabs.com/assets/cosmic-confrontation/page-cover.png" alt="Imagem de Apresenta√ß√£o"></p>

## ‚ö°Ô∏è O que √© o Cosmic Confrontation?

Um jogo de aventura 3D que passa-se em uma ilha com v√°rios cen√°rios num planeta distante, onde o jogador iniciar√° o jogo com Halley Bennet, que parte em uma aventura para encontrar um tesouro escondido antes que os piratas do universo cheguem primeiro, dessa forma para devolver a vida √† normalidade do cosmo pelos que foi conquistada pelos piratas.

Cosmic Confrontation √© um jogo desenvolvido em ambiente desktop com Unity.

> Esta n√£o √© um jogo oficializado no mercado, foi construida no √¢mbito acad√©mico de forma aumentar as nossas capacidades t√©cnicas e interpessoais.

## üí° Pr√©-Requisitos

Para comer√ßar a usar o software localmente na sua m√°quina, basta instalar o [Unity](https://unity.com/pt/download). Foi utilizada a vers√£o ```2022.3.10f1```, caso queira a vers√£o exata pode encontr√°-la [aqui](https://unity.com/pt/releases/editor/archive).

## ‚öôÔ∏è Instala√ß√£o

Para colocar o projeto a funcionar localmente na sua m√°quina basta:

1. Clonar o reposit√≥rio.

2. Abrir o Unity Hub.

3. Instalar a vers√£o do Unity.

4. Abrir o projeto no Unity.

## ü§ñ 1¬∫ T√©cnica de IA - Path Finding para movimento das aranhas

###  Navmesh: 

O objetivo pretendido seria que as aranhas se deslocassem pelo mapa em diferentes dire√ß√µes, com uma velocidade e dist√¢ncia constante. Para isso foi utilizada a solu√ß√£o de pathfinding do unity chamada de Navmesh, que foi adicionada √† superf√≠cie e foi realizado bake da mesma, o que permitiu criar assim uma mesh de navega√ß√£o com a √°rea pela qual a aranha pode andar.  De forma a permitir que a aranha circule pela cena utilizando a Navmesh foi necess√°rio adicionar um componente √† mesma, chamado de Navmesh Agent. 

###  Implementa√ß√£o:

A n√≠vel de c√≥digo foram utilizadas tr√™s fun√ß√µes, sendo elas o Start, Update e SetRandomTarget.

Quando inicia a cena √© chamada a fun√ß√£o SetRandomTarget() para o movimento das aranhas, que faz a verifica√ß√£o atrav√©s de um if else. Se a aranha est√° a andar para a frente, a posi√ß√£o target √© definida como a posi√ß√£o inicial da aranha mais um ponto aleat√≥rio da esfera multiplicado pela dist√¢ncia m√°xima definida. Se n√£o estiver a mover para a frente, a posi√ß√£o target √© definida como sendo igual √† inicial.

Na fun√ß√£o Update que vai sendo atualizada a cada frame, √© feita a verifica√ß√£o atrav√©s de um if, se a dist√¢ncia restante para a posi√ß√£o alvo √© inferior a 0.1 e se for chama a fun√ß√£o SetRandomTarget para calcular um novo ponto de destino. Desta forma a aranha ir√° sempre andar em diferentes dire√ß√µes. 

#### Ficheiros utilizados

O c√≥digo relativo ao **desenvolvimento da t√©cnica** encontra-se em [Assets/Scripts/SpiderAI.cs](Assets/Scripts/SpiderAI.cs).

### Demonstra√ß√£o

Criamos uma pequeno **trailer** para mostrar o funcionamento do pathfinding das aranhas.
[![Demonstra√ß√£o](https://img.youtube.com/vi/0DWwADyst5Y/0.jpg)](https://www.youtube.com/watch?v=0DWwADyst5Y)

## ü§ñ 2¬∫ T√©cnica de IA - M√°quina de Estados para tomada de decis√£o dos inimigos

Para este t√≥pico, foi criada uma m√°quina de estados, que faz a transi√ß√£o entre 5 estados:
Idle, patrulhar, atacar parado, atacar a caminhar e morrer. Para cada um destes estados foram criadas uma classe para cada estado.
Estas classes foram criadas a partir da baseState, que √© a classe da qual herdam os m√©todos principais da m√°quina de estados: Enter, Update e Exit.
Tamb√©m foi criado uma classe cahamda stateManager, que vai gerir a transi√ß√£o entre estados. Esta classe √© a super classe da EnemyStateMachine, na qual vai injectar as features do enemy nos diferentes estados.
A execu√ß√£o deste c√≥digo acontece na classe EnemyScript.

Al√©m disso, tamb√©m √© executado um script chamado EnemyGroupScript que permite mudar estado para diferentes enemies ao mesmo tempo.

Scripts utilizados/criados:
- BaseState
- EnemyStateMachine
- StateManager
- AttackChaseEnemy
- AttackIdleEnemy
- DeathEnemy
- IdleEnemy
- PatrolEnemy
- EnemyGroupScript

O c√≥digo encontra-se na branch [ai-state-machine](https://github.com/luispereira1999/cosmic-confrontation/tree/ai-state-machine)


---


## ü§ñ 3¬∫ T√©cnica de IA - Aprendizado por Refor√ßo para resolver um puzzle

### Enquadramento

No decorrer do jogo produzido na unidade curricular de Projeto Aplicado, decidimos fazer uma adapta√ß√£o para uso da t√©cnica de intelig√™ncia artificial de aprendizado por refor√ßo na unidade curricular de Intelig√™ncia Artificial Aplicada a Jogos.\
Escolhemos esta t√©cnica, pelo facto de j√° termos um puzzle no nosso jogo e, a ideia que o jogador treinado com IA atrav√©s de tentativa-erro para **resolver um puzzle** e aprender consoante o ambiente que est√° exposto, enquadra-se bem na sua aplica√ß√£o.\
Instalou-se o [Python 3.9.13](https://www.python.org/downloads/release/python-3913) para criar um ambiente, e usufruiu-se principalmente da biblioteca [ML-Agents](https://github.com/Unity-Technologies/ml-agents) que permitiu realizar o treinamento e o desenvolvimento dos agentes inteligentes no [Unity](https://unity.com/pt).

### Demonstra√ß√£o

Criamos uma pequeno **trailer** para mostrar o funcionamento da t√©cnica de aprendizado por refor√ßo.
[![Demonstra√ß√£o](https://img.youtube.com/vi/1awGiTleCNA/0.jpg)](https://youtu.be/1awGiTleCNA)

No **treino** obtivemos os resultados abaixo:\
O gr√°fico "Cumulative Reward" refere-se √† soma acumulada das recompensas pelos agentes ao longo do tempo, se a curva subir corresponde ao objetivo central, maximizar a recompensa. O gr√°fico "Episode Length" indica a dura√ß√£o das a√ß√µes que os agentes tomam em um epis√≥dio, se a linha decrescer significa que ao longo do tempo aprende as tarefas mais eficiente.\
Atrav√©s destas 2 m√©tricas conclu√≠mos que o modelo treinado apresenta bons resultados.

<img height="400" src="Assets/SolvePuzzleAI/TrainingResults/TrainGraph.png">

### Implementa√ß√£o

Primeiro de tudo, recorremos aos conte√∫dos disponibilizadas pelo docente e √†s informa√ß√µes contidas na internet para abordar a teoria por tr√°s do aprendizado por refor√ßo.\
Foram utilizados 2 agentes que tomam decis√µes, um para escolher a 1¬∫ pe√ßa e outro para escolher a 2¬∫ pe√ßa, e no fim as pe√ßas trocam de lugar no puzzle.
Durante o treinamento usou-se um n√∫mero m√°ximo de 30 tentativas, cada uma envolvendo a troca de 2 pe√ßas, se n√£o resolvesse o puzzle nessas tentativas, perdia e recome√ßava de novo, para assim acrescentar mais um indicador para melhor ser o treino.

#### Ficheiros utilizados

O c√≥digo relativo ao **desenvolvimento da t√©cnica** encontra-se em [Assets/SolvePuzzleAI/Scripts](Assets/SolvePuzzleAI/Scripts).\
As **cenas** de jogo para visualizar os resultados (uma para treinar outra com os modelos j√° treinados) encontra-se em [Assets/SolvePuzzleAI/Scenes](Assets/SolvePuzzleAI/Scenes).\
Os **modelos treinados** para cada agente encontra-se em [Assets/SolvePuzzleAI/ModelsAI](Assets/SolvePuzzleAI/ModelsAI).\
O **ficheiro de configura√ß√£o** para executar o treinamento encontra-se em [Config/trainconfig.yaml](Config/trainconfig.yaml).\
A pasta dos **resultados do treinamento** encontra-se em [Assets/SolvePuzzleAI/TrainingResults](Assets/SolvePuzzleAI/TrainingResults).

#### Observa√ß√µes

As observa√ß√µes s√£o fundamentais ao fornecerem informa√ß√µes sobre o estado do ambiente aos agentes do aprendizado. Funcionam como sensores para que os agentes tomem decis√µes e interajam com o ambiente. Na capacidade dos agentes aprenderem utilizamos as seguintes observa√ß√µes:

- Posi√ß√£o atual das pe√ßas no mundo/no puzzle (posi√ß√£o no eixo x,y,z/n√∫meros entre 1 a 9);
- Agente atual (usamos 2 agentes: 1 para escolher a 1¬∫ pe√ßa e o outro para a 2¬∫ pe√ßa);
- √öltima pe√ßa escolhida (n√∫mero escolhido).

#### Recompensas

As recompensas s√£o o feedback sobre o desempenho das a√ß√µes executadas pelos agentes. O objetivo √© motivar o agente a aprender comportamentos que levem a resultados positivos e sofrer penalidades em a√ß√µes que n√£o o levam ao resultado pretendido, neste caso solucionar o puzzle.

- Troca correta das pe√ßas: +10
- Troca errada das pe√ßas: -10
- 1¬∫ pe√ßa escolhida diferente da 2¬∫ pe√ßa escolhida: +2
- 1¬∫ pe√ßa escolhida igual √† 2¬∫ pe√ßa escolhida: -2
- Puzzle resolvido: +50
- Puzzle n√£o resolvido (se as tentativas acabaram): -50

#### Configura√ß√£o de ambiente de treino

Abrir a linha de comandos e executar os seguintes comandos: ir para a pasta raiz do projeto:

```sh
cd <caminho_pasta_raiz_do_projeto>         # v√° para a pasta raiz do projeto
python -m venv <nome_do_ambiente>          # cria um novo ambiente
<nome_do_ambiente>\Scripts\activate        # Abre o ambiente criado
python -m pip install --upgrade pip        # instala os pacotes necess√°rios:
pip3 install mlagents
pip3 install torch torchvision torchaudio
pip install protobuf==3.20.3
pip install onnx
pip install packaging
mlagents-learn Config\trainconfig.yaml --run-id=<nome_do_treino>    # inicia o treinamento
tensorboard --logdir results                                        # exibe resultados em gr√°ficos
```

Depois do treino acabar, os ficheiros ser√£o armazenados numa pasta chamada `results` na raiz do projeto!

## üëç Contribui√ß√µes

As contribui√ß√µes s√£o o que tornam a comunidade de c√≥digo aberto um lugar incr√≠vel para aprender, inspirar e criar. Quaisquer contribui√ß√µes que voc√™ fa√ßa s√£o muito apreciadas.

Se voc√™ tiver uma sugest√£o de melhoria, por favor, fa√ßa fork do reposit√≥rio e crie uma pull request. Ou pode simplesmente abrir um issue. N√£o se esque√ßa de dar uma estrela ao projeto! Obrigado mais uma vez!

## ‚≠êÔ∏è Colaboradores

- Lu√≠s Pereira
- Pedro Silva
- V√¢nia Pereira

## ‚ö†Ô∏è Licen√ßa

Ao contribuir para este projeto, voc√™ concorda com as pol√≠ticas da licen√ßa [MIT](LICENSE).
